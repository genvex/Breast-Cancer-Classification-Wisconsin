{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Breast Cancer Classification_Clean",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg5c00Qa8jJD",
        "colab_type": "text"
      },
      "source": [
        "# **Breast Cancer Classification**\n",
        "\n",
        "**Author:** Meg Hutch\n",
        "\n",
        "**Date:** October 22, 2019\n",
        "\n",
        "\n",
        "**Objective:** Classify Breast Cancer Tumnors as Malignant or Benign from the Breast Cancer Wisconin Dataset downloaded fromn https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
        "\n",
        "**Additional reference:** https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "**Dataset:** The following is the given data descriptions: \n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "1.   ID Number\n",
        "2.   Diagnosis (M = malignant, B = benign)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus (3-32):\n",
        "\n",
        "3.  radius (mean of distances from center to points on the perimeter)\n",
        "4.  texture (standard deviation of gray-scale values)\n",
        "5.  perimeter\n",
        "6.  area\n",
        "7.  smoothness (local variation in radius lengths)\n",
        "8.  compactness (perimeter^2 / area - 1.0)\n",
        "9.  concavity (severity of concave portions of the contour)\n",
        "10. concave points (number of concave portions of the contour)\n",
        "11. symmetry\n",
        "12. fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj7CQe7jKD2U",
        "colab_type": "text"
      },
      "source": [
        "**WIP Updates**\n",
        "\n",
        "**11.01.2019: MH implemented the code for logisitic regression, will need to add random forest, and also ensure the neural network is okay**\n",
        "\n",
        "**11.04.2019: MH is not sure if I need the train_x or val_x to also be converted to long format ; I'm having problems getting the models to run due to dimension problems**\n",
        "\n",
        "**11.05.2019: MH will try revising the code for xb, yb and in regards to the data loader -- I think this is the problem. First though, I will save what I've done to github**\n",
        "\n",
        "**Update: Can't figure out how to upload to github, but I created a cleaner version of this code. I'm thinking about removing the validaiton set, since I think this reduces the size, but I'm a bit weary of doing so.**\n",
        "\n",
        "**Need to also implement random forest classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEngWwvv8imx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import seaborn as sns\n",
        "\n",
        "# Connect Colab to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLEnXnzk-lqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "tumor = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Wisconsin/data.csv')\n",
        "\n",
        "# View data\n",
        "tumor.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw7TlnDfAiZH",
        "colab_type": "text"
      },
      "source": [
        "# **Explore Data**\n",
        "\n",
        "First, we will examine the worst values obtained from each patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64Itdjz_6hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor.diagnosis.value_counts().plot(kind=\"bar\")\n",
        "count_dx = tumor.groupby(['diagnosis']).size()\n",
        "print('Total Number of Patients:', len(tumor.index))\n",
        "print('Number Diagnosed:', count_dx)\n",
        "print('Percent Benign: {:.1f}'.format(357/len(tumor.index)))\n",
        "print('Percent Malignant: {:.1f}'.format(212/len(tumor.index)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiXuempE_9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new dataframe to just contain columns of interset\n",
        "tumor_plots = tumor[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "\n",
        "# Generically define how many plots along and across\n",
        "ncols = 3\n",
        "nrows = int(np.ceil(len(tumor_plots.columns) / (1.0*ncols)))\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "\n",
        "# Lazy counter so we can remove unwanted axes\n",
        "counter = 0\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "\n",
        "        ax = axes[i][j]\n",
        "\n",
        "        # Plot when we have data\n",
        "        if counter < len(tumor_plots.columns):\n",
        "\n",
        "            ax.hist(tumor_plots[tumor_plots.columns[counter]], bins=50, color='blue', alpha=0.5, label='{}'.format(tumor_plots.columns[counter]))\n",
        "            ax.set_xlabel('x')\n",
        "            ax.set_ylabel('PDF')\n",
        "            leg = ax.legend(loc='upper left')\n",
        "            leg.draw_frame(False)\n",
        "\n",
        "        # Remove axis when we no longer have data\n",
        "        else:\n",
        "            ax.set_axis_off()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzb8q0UFN8Gz",
        "colab_type": "text"
      },
      "source": [
        "# **Correlations for Feature Selection**\n",
        "\n",
        "I'll eventually have to learn how to look into this more, as of now, I'm just going to include all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiyLMuzwIBoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic correlogram\n",
        "#sns.pairplot(tumor_plots)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js2xeo28UYjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGMu8lrOT3NP",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "We will first try and assess classification using a simple logistic regression - this will also serve as a bench mark once we develop our neural network classifier\n",
        "\n",
        "These steps were followed from the following tutorial: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_QNUYJUEeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packages \n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-XsYtW6VGiZ",
        "colab_type": "text"
      },
      "source": [
        "Create dummy variables for diagnoses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx-r5-39f3oS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor['diagnosis'] = tumor.diagnosis.map({'B':0, 'M':1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zX8HZEAUpr0",
        "colab_type": "text"
      },
      "source": [
        "Create data frames into features and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saU_eKbKUPJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create x to represent the input features; y is the label; \n",
        "x = tumor[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "y = tumor.diagnosis # This is output of our training data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaRAWZErUmWK",
        "colab_type": "text"
      },
      "source": [
        "Split the data into testing and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcRht6w7Ukfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcupQtxvUv4Y",
        "colab_type": "text"
      },
      "source": [
        "Develop the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXGPVFGJUwry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXQpnqyzVAo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_5ROBDWlZ1",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaulation using Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E6ZqC4EW4u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQuiYl2CXDfn",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix generated abouve is in the form of an array. Diagonal values represent accurate predictions, while non-diagnonal elements are inaccurate predictions. The diagnoal starting with the top left to the bottom right hand corner are the actual predictions, while the bottom left corner to the top right corner are incorrect predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrgS-ANvXbks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZKVIx5-aqCY",
        "colab_type": "text"
      },
      "source": [
        "**ROC**\n",
        "\n",
        "The Reciever Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFijxjja1zW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unSP50nHbQtq",
        "colab_type": "text"
      },
      "source": [
        "# **Random Forest Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBP42op2Oaca",
        "colab_type": "text"
      },
      "source": [
        "## **PyTorch Neural Network for Classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ld6UECBRrk93",
        "colab": {}
      },
      "source": [
        "# Import PyTorch packages\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9rwndncaF9ig",
        "colab": {}
      },
      "source": [
        "# Function that randomly shuffles and splits the dataset\n",
        "def split_indices(n, val_pct):\n",
        "  # Determine size of test/validation set\n",
        "  n_val = int(val_pct*n)\n",
        "  # Create random permutation of 0 to n-1\n",
        "  idxs = np.random.permutation(n)\n",
        "  # Pick first n_val indices for test/validation set\n",
        "  return idxs[n_val:], idxs[:n_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6clf1ZkZF9ik",
        "colab": {}
      },
      "source": [
        "train_indices, test_indices = split_indices(len(tumor), val_pct=0.2)\n",
        "\n",
        "print(len(train_indices), len(test_indices))\n",
        "print('Sample test indices: ' , test_indices[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "59WMsZ17F9in",
        "colab": {}
      },
      "source": [
        "# Create a test set\n",
        "test_ds = tumor[tumor.index.isin(test_indices)]\n",
        "\n",
        "# Rename the train at tumor\n",
        "tumor = tumor[tumor.index.isin(train_indices)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RQAkOmMtF9ip"
      },
      "source": [
        "Now we can apply this function once more, to create a training and validaiton set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5dOWq1yRF9ip",
        "colab": {}
      },
      "source": [
        "train_indices, val_indices = split_indices(len(tumor), val_pct=0.2)\n",
        "\n",
        "print(len(train_indices), len(val_indices))\n",
        "print('Sample val indices: ' , val_indices[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zf-Y2-ZnF9ir"
      },
      "source": [
        "**Create a Training Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e8z-DfAjF9is",
        "colab": {}
      },
      "source": [
        "# Create training set\n",
        "train_ds = tumor[tumor.index.isin(train_indices)]\n",
        "\n",
        "# Using the training dataset just created, remove the diagnosis variable and create training feature and label vector\n",
        "xb = train_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = train_ds.diagnosis # This is output of our training data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "trainloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANLD0vRZF9iu"
      },
      "source": [
        "**Similarly, Create the Validation Set**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6nPXR2gMF9iv",
        "colab": {}
      },
      "source": [
        "# Create validation set\n",
        "val_ds = tumor[tumor.index.isin(val_indices)]\n",
        "\n",
        "# Using the validation dataset just created, remove the diagnosis variable and create validaiton feature and label vector\n",
        "xb = val_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = val_ds.diagnosis # This is output of our validation data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "val_loader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Validation Loader\n",
        "val_loader = DataLoader(val_loader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ak9tHX6MZXx",
        "colab_type": "text"
      },
      "source": [
        "**Create/Format the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4YTswc0Mo0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the test dataset just created, remove the diagnosis variable and create test feature and label vector\n",
        "xb = test_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = test_ds.diagnosis # This is output of our test data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "testloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Test Loader\n",
        "testloader = DataLoader(testloader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqdQLEFfM5eE",
        "colab_type": "text"
      },
      "source": [
        "**Create Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLZs2xTcTyXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model with one hidden layer\n",
        "model = nn.Sequential(nn.Linear(10, 5),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(5, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Set 50 epochs to start\n",
        "epochs = 50\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for xb, yb in trainloader:\n",
        "\n",
        "        # Flatten yb\n",
        "        #yb = yb.view(yb.shape[0], -1)\n",
        "        \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(xb)\n",
        "        loss = criterion(output, yb.long()) # Loss calculated from the output compared to the labels \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oY7eGLJUnlb",
        "colab_type": "text"
      },
      "source": [
        "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically, this is just accuracy, the percentage of classes the network predicted correctly. \n",
        "\n",
        "First, do a forward pass with one batch from the test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UexgheavO9L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb, yb = next(iter(testloader))\n",
        "\n",
        "# Get the class probabilities \n",
        "ps = torch.exp(model(xb))\n",
        "\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BOkgVzsVJfl",
        "colab_type": "text"
      },
      "source": [
        "With the probabilities, we can get the most likely class using the ps.topk method. This returns the k highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top-k values and the top-k indices. If the highest value is the first element, we'll get back 4 as the index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HMCtu70PVgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "# Look at the most likely classes for the first 10 examples\n",
        "print(top_class[:20,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uIABTsjVYz0",
        "colab_type": "text"
      },
      "source": [
        "Now we can check if the predicted classes match the labels. This is simple to do by equating top_class and labels, but we have to be careful of the shapes. To get the equality to work out the way we want, top_class and the labels (yb) must have the same shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPw1mTkRE5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "equals = top_class == yb.view(*top_class.shape) \n",
        "equals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H0McJLVVoi1",
        "colab_type": "text"
      },
      "source": [
        "Now we need to calculate the correct predictions. \n",
        "\n",
        "equals has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the total number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to torch.mean. \n",
        "\n",
        "So we'll need to convert equals to a float tensor. Note that when we take torch.mean it returns a scalar tensor, to get the actual value as a float we'll need to do accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiSuP4tGSPty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "print(f'Accuracy: {accuracy.item()*100}%')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Breast Cancer Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg5c00Qa8jJD",
        "colab_type": "text"
      },
      "source": [
        "# **Breast Cancer Classification**\n",
        "\n",
        "**Author:** Meg Hutch\n",
        "\n",
        "**Date:** October 22, 2019\n",
        "\n",
        "\n",
        "**Objective:** Classify Breast Cancer Tumnors as Malignant or Benign from the Breast Cancer Wisconin Dataset downloaded fromn https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
        "\n",
        "**Additional reference:** https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "**Dataset:** The following is the given data descriptions: \n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "1.   ID Number\n",
        "2.   Diagnosis (M = malignant, B = benign)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus (3-32):\n",
        "\n",
        "3.  radius (mean of distances from center to points on the perimeter)\n",
        "4.  texture (standard deviation of gray-scale values)\n",
        "5.  perimeter\n",
        "6.  area\n",
        "7.  smoothness (local variation in radius lengths)\n",
        "8.  compactness (perimeter^2 / area - 1.0)\n",
        "9.  concavity (severity of concave portions of the contour)\n",
        "10. concave points (number of concave portions of the contour)\n",
        "11. symmetry\n",
        "12. fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj7CQe7jKD2U",
        "colab_type": "text"
      },
      "source": [
        "**WIP Updates**\n",
        "\n",
        "**11.01.2019: MH implemented the code for logisitic regression, will need to add random forest, and also ensure the neural network is okay**\n",
        "\n",
        "**11.04.2019: MH is not sure if I need the train_x or val_x to also be converted to long format ; I'm having problems getting the models to run due to dimension problems**\n",
        "\n",
        "**11.05.2019: MH will try revising the code for xb, yb and in regards to the data loader -- I think this is the problem. First though, I will save what I've done to github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEngWwvv8imx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import seaborn as sns\n",
        "\n",
        "# Connect Colab to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLEnXnzk-lqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "tumor = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Wisconsin/data.csv')\n",
        "\n",
        "# View data\n",
        "tumor.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw7TlnDfAiZH",
        "colab_type": "text"
      },
      "source": [
        "# **Explore Data**\n",
        "\n",
        "First, we will examine the worst values obtained from each patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64Itdjz_6hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor.diagnosis.value_counts().plot(kind=\"bar\")\n",
        "count_dx = tumor.groupby(['diagnosis']).size()\n",
        "print('Total Number of Patients:', len(tumor.index))\n",
        "print('Number Diagnosed:', count_dx)\n",
        "print('Percent Benign: {:.1f}'.format(357/len(tumor.index)))\n",
        "print('Percent Malignant: {:.1f}'.format(212/len(tumor.index)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiXuempE_9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new dataframe to just contain columns of interset\n",
        "tumor_plots = tumor[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "\n",
        "# Generically define how many plots along and across\n",
        "ncols = 3\n",
        "nrows = int(np.ceil(len(tumor_plots.columns) / (1.0*ncols)))\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "\n",
        "# Lazy counter so we can remove unwanted axes\n",
        "counter = 0\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "\n",
        "        ax = axes[i][j]\n",
        "\n",
        "        # Plot when we have data\n",
        "        if counter < len(tumor_plots.columns):\n",
        "\n",
        "            ax.hist(tumor_plots[tumor_plots.columns[counter]], bins=50, color='blue', alpha=0.5, label='{}'.format(tumor_plots.columns[counter]))\n",
        "            ax.set_xlabel('x')\n",
        "            ax.set_ylabel('PDF')\n",
        "            leg = ax.legend(loc='upper left')\n",
        "            leg.draw_frame(False)\n",
        "\n",
        "        # Remove axis when we no longer have data\n",
        "        else:\n",
        "            ax.set_axis_off()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzb8q0UFN8Gz",
        "colab_type": "text"
      },
      "source": [
        "# **Correlations for Feature Selection**\n",
        "\n",
        "I'll eventually have to learn how to look into this more, as of now, I'm just going to include all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiyLMuzwIBoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic correlogram\n",
        "#sns.pairplot(tumor_plots)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js2xeo28UYjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGMu8lrOT3NP",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "We will first try and assess classification using a simple logistic regression - this will also serve as a bench mark once we develop our neural network classifier\n",
        "\n",
        "These steps were followed from the following tutorial: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_QNUYJUEeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packages \n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-XsYtW6VGiZ",
        "colab_type": "text"
      },
      "source": [
        "Create dummy variables for diagnoses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx-r5-39f3oS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor['diagnosis'] = tumor.diagnosis.map({'B':0, 'M':1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zX8HZEAUpr0",
        "colab_type": "text"
      },
      "source": [
        "Create data frames into features and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saU_eKbKUPJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create x to represent the input features; y is the label; \n",
        "x = tumor[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "y = tumor.diagnosis # This is output of our training data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaRAWZErUmWK",
        "colab_type": "text"
      },
      "source": [
        "Split the data into testing and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcRht6w7Ukfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcupQtxvUv4Y",
        "colab_type": "text"
      },
      "source": [
        "Develop the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXGPVFGJUwry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXQpnqyzVAo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_5ROBDWlZ1",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaulation using Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E6ZqC4EW4u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQuiYl2CXDfn",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix generated abouve is in the form of an array. Diagonal values represent accurate predictions, while non-diagnonal elements are inaccurate predictions. The diagnoal starting with the top left to the bottom right hand corner are the actual predictions, while the bottom left corner to the top right corner are incorrect predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrgS-ANvXbks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZKVIx5-aqCY",
        "colab_type": "text"
      },
      "source": [
        "**ROC**\n",
        "\n",
        "The Reciever Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFijxjja1zW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unSP50nHbQtq",
        "colab_type": "text"
      },
      "source": [
        "# **Random Forest Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBP42op2Oaca",
        "colab_type": "text"
      },
      "source": [
        "## **PyTorch Neural Network for Classification**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ld6UECBRrk93",
        "colab": {}
      },
      "source": [
        "# Import PyTorch packages\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_EC7v-D7yV",
        "colab_type": "text"
      },
      "source": [
        "Meg is going to restart here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9rwndncaF9ig",
        "colab": {}
      },
      "source": [
        "# Function that randomly shuffles and splits the dataset\n",
        "def split_indices(n, val_pct):\n",
        "  # Determine size of test/validation set\n",
        "  n_val = int(val_pct*n)\n",
        "  # Create random permutation of 0 to n-1\n",
        "  idxs = np.random.permutation(n)\n",
        "  # Pick first n_val indices for test/validation set\n",
        "  return idxs[n_val:], idxs[:n_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6clf1ZkZF9ik",
        "colab": {}
      },
      "source": [
        "train_indices, test_indices = split_indices(len(tumor), val_pct=0.2)\n",
        "\n",
        "print(len(train_indices), len(test_indices))\n",
        "print('Sample test indices: ' , test_indices[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "59WMsZ17F9in",
        "colab": {}
      },
      "source": [
        "# Create a test set\n",
        "test_ds = tumor[tumor.index.isin(test_indices)]\n",
        "\n",
        "# Rename the train at tumor\n",
        "tumor = tumor[tumor.index.isin(train_indices)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RQAkOmMtF9ip"
      },
      "source": [
        "Now we can apply this function once more, to create a training and validaiton set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5dOWq1yRF9ip",
        "colab": {}
      },
      "source": [
        "train_indices, val_indices = split_indices(len(tumor), val_pct=0.2)\n",
        "\n",
        "print(len(train_indices), len(val_indices))\n",
        "print('Sample val indices: ' , val_indices[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zf-Y2-ZnF9ir"
      },
      "source": [
        "**Create a Training Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e8z-DfAjF9is",
        "colab": {}
      },
      "source": [
        "# Create training set\n",
        "train_ds = tumor[tumor.index.isin(train_indices)]\n",
        "\n",
        "# Using the training dataset just created, remove the diagnosis variable and create training feature and label vector\n",
        "xb = train_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = train_ds.diagnosis # This is output of our training data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "trainloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ANLD0vRZF9iu"
      },
      "source": [
        "**Similarly, Create the Validation Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6nPXR2gMF9iv",
        "colab": {}
      },
      "source": [
        "# Create validation set\n",
        "val_ds = tumor[tumor.index.isin(val_indices)]\n",
        "\n",
        "# Using the validation dataset just created, remove the diagnosis variable and create validaiton feature and label vector\n",
        "xb = val_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = val_ds.diagnosis # This is output of our validation data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "val_loader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Validation Loader\n",
        "val_loader = DataLoader(val_loader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ak9tHX6MZXx",
        "colab_type": "text"
      },
      "source": [
        "**Create/Format the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4YTswc0Mo0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the test dataset just created, remove the diagnosis variable and create test feature and label vector\n",
        "xb = test_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = test_ds.diagnosis # This is output of our test data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "testloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Test Loader\n",
        "testloader = DataLoader(testloader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqdQLEFfM5eE",
        "colab_type": "text"
      },
      "source": [
        "**Create Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLZs2xTcTyXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model with one hidden layer\n",
        "model = nn.Sequential(nn.Linear(10, 5),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(5, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Set 20 epochs to start\n",
        "epochs = 20\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for xb, yb in trainloader:\n",
        "\n",
        "        # Flatten yb\n",
        "        #yb = yb.view(yb.shape[0], -1)\n",
        "        \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(xb)\n",
        "        loss = criterion(output, yb.long()) # Loss calculated from the output compard to the labels \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slbhzRpGL4G-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/3bd7dea850e936d8cb44adda8200e4e2b5d627e3/intro-to-pytorch/helper.py\n",
        "import helper"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UexgheavO9L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb, yb = next(iter(testloader))\n",
        "\n",
        "# Get the class probabilities \n",
        "ps = torch.exp(model(xb))\n",
        "\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HMCtu70PVgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "# Look at the most likely classes for the first 10 examples\n",
        "print(top_class[:20,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPw1mTkRE5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "equals = top_class == yb.view(*top_class.shape) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw9uzs8rSL1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "equals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiSuP4tGSPty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "print(f'Accuracy: {accuracy.item()*100}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-m1y1aOD4v_",
        "colab_type": "text"
      },
      "source": [
        "# **DRAFT!!!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmJ1hSs71hm1",
        "colab_type": "text"
      },
      "source": [
        "First need to create a test set. For this we will choose 20% of the data and define the function that can split the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPxNmmBE1jdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that randomly shuffles and splits the dataset\n",
        "def split_indices(n, val_pct):\n",
        "  # Determine size of test/validation set\n",
        "  n_val = int(val_pct*n)\n",
        "  # Create random permutation of 0 to n-1\n",
        "  idxs = np.random.permutation(n)\n",
        "  # Pick first n_val indices for test/validation set\n",
        "  return idxs[n_val:], idxs[:n_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JIfNbAWg2OOc",
        "colab": {}
      },
      "source": [
        "train_indices, test_indices = split_indices(len(tumor), val_pct=0.2)\n",
        "\n",
        "print(len(train_indices), len(test_indices))\n",
        "print('Sample test indices: ' , test_indices[:20])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzKTigp52Vm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a test set\n",
        "test_ds = tumor[tumor.index.isin(test_indices)]\n",
        "\n",
        "# Rename the train at tumor\n",
        "tumor = tumor[tumor.index.isin(train_indices)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZikbGdq248g",
        "colab_type": "text"
      },
      "source": [
        "Now we can apply this function once more, to create a training and validaiton set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhJUGQTjGANQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_indices, val_indices = split_indices(len(tumor), val_pct=0.2)\n",
        "\n",
        "print(len(train_indices), len(val_indices))\n",
        "print('Sample val indices: ' , val_indices[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGWYcZp6nWML",
        "colab_type": "text"
      },
      "source": [
        "**Create a Training Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSdUkwInZlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# Create training set\n",
        "train_ds = tumor[tumor.index.isin(train_indices)]\n",
        "\n",
        "# Using the training dataset just created, remove the diagnosis variable and create training feature and label vector\n",
        "xb = train_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = train_ds.diagnosis # This is output of our training data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Convert to longform\n",
        "xb = xb.long()\n",
        "yb = yb.long()\n",
        "\n",
        "#Combine the arrays \n",
        "trainloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0BB97a5n35q",
        "colab_type": "text"
      },
      "source": [
        "**Similarly, Create the Validation Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BaJkgnTn9qP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "# Create validation set\n",
        "val_ds = tumor[tumor.index.isin(val_indices)]\n",
        "\n",
        "# Using the validation dataset just created, remove the diagnosis variable and create validaiton feature and label vector\n",
        "xb = val_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "yb = val_ds.diagnosis # This is output of our validation data\n",
        "\n",
        "# Convert data into arrays\n",
        "xb = np.array(xb, dtype = \"float32\")\n",
        "yb = np.array(yb, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "# Convert to longform\n",
        "xb = xb.long()\n",
        "yb = yb.long()\n",
        "\n",
        "#Combine the arrays \n",
        "val_loader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Validation Loader\n",
        "val_loader = DataLoader(val_loader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-BiJRxYBp1tF"
      },
      "source": [
        "# **Building the Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Za6aImiRp1tI",
        "colab": {}
      },
      "source": [
        "input_size = 10 # we have 10 input features \n",
        "num_classes = 2 # we have two output labels, benign and melignant\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes) # nn.Linear can automatically intialize the weights and biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cvmc7RQep1tK",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t8_Tfx9wp1tM",
        "colab": {}
      },
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YmuWk8x-p1tQ",
        "colab": {}
      },
      "source": [
        "for xb, xy in trainloader:\n",
        "  print(xb.shape)\n",
        "  outputs = model(xb)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hjxLuxq4p1tS",
        "colab": {}
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "  def __init__(self): # we instantiate the weights and biases\n",
        "    super().__init__() \n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "    \n",
        "  def forward(self,xb): # we flatten out the input tensor, and then pass it into self.linear\n",
        "    #xb = xb.reshape(-1, 10) # indicates to PyTorch that we want a view of the xb tensor with two dimensions, where the length along the 2nd dimension is 10 - on argument to .reshape can be set to -1 (in this case, the first dimension), to let PyTorch figure it out automatically based on the shape of the original tensor\n",
        "    out = self.linear(xb)\n",
        "    return out\n",
        "  \n",
        "model = MnistModel() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkACI1ZDp1tU",
        "colab": {}
      },
      "source": [
        "for xb, xy in trainloader:\n",
        "    outputs = model(xb)\n",
        "    break\n",
        "\n",
        "print('outputs.shape :', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:25].data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwbLvWcup1tV",
        "colab": {}
      },
      "source": [
        "# Apply softmax for each output row\n",
        "probs = F.softmax(outputs, dim=1) # The softmax function requires us to specify a dimension along which the softmax must be applied\n",
        "\n",
        "# Look at sample probabilities\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "# Add up the probabilities of an output row\n",
        "print(\"Sum:\", torch.sum(probs[0]).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gZ10Exy1p1tY"
      },
      "source": [
        "Finally, we can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. This is done using torch.max, which returns the largest element and the index of the largest element along a particular dimension of a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FCGUkRS0p1tY",
        "colab": {}
      },
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nsKQ8H6op1ta"
      },
      "source": [
        "Now we can compare with the actual labels (MH thinks this should be with the train_y dataset since we are seeing whether the model was able to learn from the train_x set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fsumLwNYp1tb",
        "colab": {}
      },
      "source": [
        "yb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FhXT2GZSp1td"
      },
      "source": [
        "# **Evaluation Metric and Loss Function**\n",
        "\n",
        "We need a way to evaluate how well our model is performing. For this reason, we can use cross entropy. \n",
        "\n",
        "Cross Entropy works as follows: \n",
        "\n",
        "* For each output row, pick the predicted probability for the correct label. E.g. if the predicted probabilities for an image are [0.1, 0.3, 0.2,...] and the correct label is 1, we pick the corresponding element 0.3 and ignore the rest.\n",
        "* Then, take the logarithm of the picked probability. If the probability is high i.e. close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiple the result by -1, which results in a large positive value of the loss for poor predictions. \n",
        "* Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data. \n",
        "\n",
        "Cross-entropy is continuous and differentiable that provides a good feedback for incremental improvments in the model (a slightly higher probability for the correct label leads to a lower loss). \n",
        "\n",
        "PyTorch provides an efficient and tensor-friendly implementation of cross-entropy. It also performs a softmax internally so we can directly pass in the outputs of the model without converting them into probabilities. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ed-txb-GqXqy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bAUNwI9MqXrG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E1HP6w5uqXrH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QyxVS2MGqXrM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LTcjjL0sqXrN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rmkkAHanqXrN",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "70nqmpLxqXrP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ORYp8_spqXrQ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WflcUJFEqXrS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lwPCcnLLqXrS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "93LjUITZqXrV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "both",
        "id": "dEI5RyCkqXrW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZTWrWxSqsYOH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWnZa91YnSaY",
        "colab_type": "text"
      },
      "source": [
        "# **DRAFTS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6WxLsxMjPYu",
        "colab_type": "text"
      },
      "source": [
        "Create training and validation sets for the data and then apply the indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekBQWZJ93rRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training set\n",
        "train_ds = tumor[tumor.index.isin(train_indices)]\n",
        "\n",
        "# Create a validation set\n",
        "val_ds = tumor[tumor.index.isin(val_indices)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "044h1kVJ_PJF",
        "colab_type": "text"
      },
      "source": [
        "Create data loaders for the training and validation set. But first, remove the diagnosis variable from the x sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xceuSMjI_b4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using the training dataset just created, remove the diagnosis variable and create training feature and label vector\n",
        "train_x = train_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "train_y = train_ds.diagnosis # This is output of our training data\n",
        "\n",
        "# do the same with the testing set\n",
        "val_x = val_ds[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']] # taking test data inputs\n",
        "val_y = val_ds.diagnosis   #output value of test dat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkVSfeMj03c",
        "colab_type": "text"
      },
      "source": [
        "Convert data into arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYc0xzNGDJAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "train_x = np.array(train_x, dtype = \"float32\")\n",
        "train_y = np.array(train_y, dtype= \"float32\")\n",
        "\n",
        "val_x = np.array(val_x, dtype = \"float32\")\n",
        "val_y = np.array(val_y, dtype= \"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPtSKCZCrGp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(train_x)\n",
        "#len(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slPsVx1XDcnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert arrays into tensors\n",
        "train_x = torch.from_numpy(train_x)\n",
        "train_y = torch.from_numpy(train_y)\n",
        "\n",
        "val_x = torch.from_numpy(val_x)\n",
        "val_y = torch.from_numpy(val_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srYDIymYrv9l",
        "colab_type": "text"
      },
      "source": [
        "Now we can create the dataloader "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFHHkTtVC7KD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#Combine the arrays \n",
        "trainloader = TensorDataset(train_x, train_y) \n",
        "val_loader = TensorDataset(val_x, val_y) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFKCSdEDOg42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, \n",
        "                         batch_size)\n",
        "\n",
        "# Validation Data Loader\n",
        "val_loader = DataLoader(val_loader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-tDglWTsja-",
        "colab_type": "text"
      },
      "source": [
        "# **Building the Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv485_TIsi0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 10 # we have 10 input features \n",
        "num_classes = 2 # we have two output labels, benign and melignant\n",
        "\n",
        "# Logistic regression model\n",
        "model = nn.Linear(input_size, num_classes) # nn.Linear can automatically intialize the weights and biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYk1FolJsuP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrJFiloms2Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wv_ZfKVtF2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_x, train_y in trainloader:\n",
        "  print(train_x.shape)\n",
        "  outputs = model(train_x)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOUlFS2twpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "  def __init__(self): # we instantiate the weights and biases\n",
        "    super().__init__() \n",
        "    self.linear = nn.Linear(input_size, num_classes)\n",
        "    \n",
        "  def forward(self, train_x): # we flatten out the input tensor, and then pass it into self.linear\n",
        "    #train_x = train_x.reshape(-1, 10) # indicates to PyTorch that we want a view of the xb tensor with two dimensions, where the length along the 2nd dimension is 10 - on argument to .reshape can be set to -1 (in this case, the first dimension), to let PyTorch figure it out automatically based on the shape of the original tensor\n",
        "    out = self.linear(train_x)\n",
        "    return out\n",
        "  \n",
        "model = MnistModel() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4JLdQRStwxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for train_x, train_y in trainloader:\n",
        "    outputs = model(train_x)\n",
        "    break\n",
        "\n",
        "print('outputs.shape :', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:25].data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPzMyf8lt-ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply softmax for each output row\n",
        "probs = F.softmax(outputs, dim=1) # The softmax function requires us to specify a dimension along which the softmax must be applied\n",
        "\n",
        "# Look at sample probabilities\n",
        "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
        "\n",
        "# Add up the probabilities of an output row\n",
        "print(\"Sum:\", torch.sum(probs[0]).item())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIdgCu0RQLj-",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. This is done using torch.max, which returns the largest element and the index of the largest element along a particular dimension of a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW8i3DhYuKz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_probs, preds = torch.max(probs, dim=1)\n",
        "print(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUyPrMFZQRp-",
        "colab_type": "text"
      },
      "source": [
        "Now we can compare with the actual labels (MH thinks this should be with the train_y dataset since we are seeing whether the model was able to learn from the train_x set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG48xRz9uKyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd-A7M_YuO6s",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation Metric and Loss Function**\n",
        "\n",
        "We need a way to evaluate how well our model is performing. For this reason, we can use cross entropy. \n",
        "\n",
        "Cross Entropy works as follows: \n",
        "\n",
        "* For each output row, pick the predicted probability for the correct label. E.g. if the predicted probabilities for an image are [0.1, 0.3, 0.2,...] and the correct label is 1, we pick the corresponding element 0.3 and ignore the rest.\n",
        "* Then, take the logarithm of the picked probability. If the probability is high i.e. close to 1, then its logarithm is a very small negative value, close to 0. And if the probability is low (close to 0), then the logarithm is a very large negative value. We also multiple the result by -1, which results in a large positive value of the loss for poor predictions. \n",
        "* Finally, take the average of the cross entropy across all the output rows to get the overall loss for a batch of data. \n",
        "\n",
        "Cross-entropy is continuous and differentiable that provides a good feedback for incremental improvments in the model (a slightly higher probability for the correct label leads to a lower loss). \n",
        "\n",
        "PyTorch provides an efficient and tensor-friendly implementation of cross-entropy. It also performs a softmax internally so we can directly pass in the outputs of the model without converting them into probabilities. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao2__MqEuKwW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = F.cross_entropy\n",
        "#loss_fn = torch.nn.BCELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiQ5sFH_5v0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the torches to long format - this seems to enable the loss_fn below\n",
        "train_x = train_x.long()\n",
        "train_y = train_y.long()\n",
        "\n",
        "val_x = val_x.long()\n",
        "val_y = val_y.long()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFhktJxYveQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss for current batch of data\n",
        "loss = loss_fn(outputs, train_y)\n",
        "print(loss) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M04nM4xH-_uS",
        "colab_type": "text"
      },
      "source": [
        "Since the cross entropy is the negative logarithm of the predicted probability of the correct label averaged over all training samples, one way to interpret the resulting number for example 7.6, is to look at e^-7.6 (0.0005), as the predicted probability of the correct label on average.\n",
        "\n",
        "The lower the loss, the better the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlgInUzq_AEo",
        "colab_type": "text"
      },
      "source": [
        "# **Optimizer**\n",
        "\n",
        "we can use the optim.SGD to update the weights and biases during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuxGCYKiATyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), \n",
        "                            lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY7I8r6PAchF",
        "colab_type": "text"
      },
      "source": [
        "Parameters like batch size and learning rate, etc, need to be picked in advance while training machine learning models and are called hyperparametesr. Picking the right hyperparameters is critical for training an accurate model within a reasonable amount of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIsXve4XAq9x",
        "colab_type": "text"
      },
      "source": [
        "# **Training the Model**\n",
        "\n",
        "After defining the data loaders, model, loss function and optimizer, we are ready to train the model. \n",
        "\n",
        "First, we can begin by defining a function loss_batch which:\n",
        "\n",
        "\n",
        "*   Calculates the loss for a batch of data\n",
        "*   Optionally performs the gradient descent update step in an optimizer is provided\n",
        "*   Optionally computes a metric (e.g. accuracy) using the predictions and actual targets \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCZ-INGcBZi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_batch(model, loss_func, train_x, train_y,\n",
        "               opt=None, metric=None):\n",
        "\n",
        "  # Calculate loss\n",
        "  preds = model(train_x)\n",
        "  loss = loss_func(preds, train_y)\n",
        "  \n",
        "  if opt is not None: # The optimizer is an optional arguement, to ensure that we can reuse loss_batch for computing the loss on the validation set\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "    # Update parameters\n",
        "    opt.step()\n",
        "    # Reset gradients\n",
        "    opt.zero_grad()\n",
        "    \n",
        "  metric_result = None\n",
        "  if metric is not None:\n",
        "      # Compute the metric\n",
        "      metric_result = metric(preds, train_y) # Computes the accuracy\n",
        "      \n",
        "  return loss.item(), len(train_x), metric_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcegvy04CtCC",
        "colab_type": "text"
      },
      "source": [
        "Next, define a function evaluate, which calculates the overall loss (and a metric if provided) for the validaiton set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_jdWfzWC78n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
        "  with torch.no_grad(): # no_grad indicates to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases\n",
        "    # Pass each batch through the model\n",
        "    results = [loss_batch(model, loss_fn, \n",
        "                          val_x, val_y, metric=metric)\n",
        "              for val_x, val_y in valid_dl]\n",
        "    # Separate losses, counts and metrics\n",
        "    losses, nums, metrics = zip(*results)\n",
        "    # Total size of the dataset\n",
        "    total = np.sum(nums)\n",
        "    # Avg. loss across batches\n",
        "    total_loss = np.sum(np.multiply(losses, nums))\n",
        "    avg_loss = total_loss / total\n",
        "    avg_metric = None\n",
        "    if metric is not None: \n",
        "      # Avg. of metric across batches\n",
        "      tot_metric = np.sum(np.multiply(metrics, nums))\n",
        "      avg_metric = tot_metric / total\n",
        "  return avg_loss, total, avg_metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkQ5yzTZEOi4",
        "colab_type": "text"
      },
      "source": [
        "Also need to redefine the accuracy to operate on an entire batch of outputs directly, so that we can use it as a metric in fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dz_hLPPFYtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(outputs, train_y):\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  return torch.sum(preds == train_y).item() / len(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jLGnS2sFlz6",
        "colab_type": "text"
      },
      "source": [
        "Note: We don't need to apply softmax to the outputs since it doesn't change the relative order of the results. This is because e^x is an increasing function. if y1 > y2, then e^y1 > e^y2 and the same holds true averaging out the values to get the softmax \n",
        "\n",
        "Examine how the model performs on the validation set with the initital set of weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG6ePIE_IAse",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "val_loss, total, val_acc = evaluate(\n",
        "    model, loss_fn, val_loader, metric=accuracy)\n",
        "print('Loss: {:.4f}, Accuracy: {:.4f}'\n",
        "     .format(val_loss, val_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzF7DLV8aX6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "type(loss_fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NomXcYoSMBXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x.type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPzyFfL9IqL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_y.numel()\n",
        "train_x.numel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbuyL9uUAMTD",
        "colab_type": "text"
      },
      "source": [
        "# **#DRAFTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1KiNz6JnYmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a model\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(10, 5)\n",
        "      self.fc2 = nn.Linear(5, 2)\n",
        "      \n",
        "  def forward(self, x):\n",
        "      # make sure input tensor is flattened\n",
        "      #x = train_x.view(train_x.shape[0], -1)\n",
        "      #x = train_x.reshape(-1, 10)\n",
        "      \n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.log_softmax(self.fc2(x), dim=1)\n",
        "      \n",
        "      return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4mRo3eHouKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "    \n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension\n",
        "    \"\"\"\n",
        "    super(TwoLayerNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H) \n",
        "    self.linear2 = nn.Linear(H, D_out)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must \n",
        "    return a Variable of output data. We can use Modules defined in the \n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_relu = F.relu(self.linear1(x))\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBSlPkJRo68T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# N is batch size; D_in is input dimension;\n",
        "# H is the dimension of the hidden layer; D_out is output dimension.\n",
        "N, D_in, H, D_out = 10, 10, 5, 2\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
        "#x = Variable(torch.randn(N, D_in))  # dim: 32 x 100\n",
        "\n",
        "x = train_x.reshape(10, 10)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = TwoLayerNet(D_in, H, D_out)\n",
        "\n",
        "# Forward pass: Compute predicted y by passing x to the model\n",
        "y_pred = model(x)   # dim: 32 x 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuKwTgEzOjWg",
        "colab_type": "text"
      },
      "source": [
        "Run the model...not sure if running into problems due to the vector needing to be flattened or problems with the trainloader? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8tfx-KTsRs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = train_x.reshape(-1, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Bxn0T9sTaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvgezWi7nmZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Classifier()\n",
        "\n",
        "train_x, train_y = next(iter(testloader))\n",
        "\n",
        "# Get the class probabilities \n",
        "ps = torch.exp(model(train_x))\n",
        "\n",
        "# Make sure the shape is appropriate\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFMCdTYoCYty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model with one hidden layer\n",
        "model = nn.Sequential(nn.Linear(10, 5),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(5, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Set 5 epochs to start\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for train_x, train_y in trainloader:\n",
        "      # Flatten data into a vector \n",
        "        #train_x = train_x.view(train_x.shape[0], -1)\n",
        "        #try the reshape method\n",
        "        #train_x = train_x.reshape(-1, 10)\n",
        "        \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(train_x)\n",
        "        loss = criterion(output, train_y) # Loss calculated from the output compard to the labels \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKO8xQdsCY1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x.numel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-y11LyCY7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9_ynGRUCY46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-v9SMPJoR0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=100\n",
        "\n",
        "# Training sampler and data loader\n",
        "train_sampler = SubsetRandomSampler(train_indices) #will sample elements randomly from a given list of indices, while creating batches of data\n",
        "train_loader = DataLoader(tumor, \n",
        "                         batch_size,\n",
        "                         sampler=train_sampler)\n",
        "\n",
        "# Validation sampler and data loader\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "val_loader = DataLoader(tumor, \n",
        "                       batch_size, \n",
        "                       sampler=val_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaGEMqgu2O9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = train[prediction_var]# taking the training data input \n",
        "train_y = train.diagnosis # This is output of our training data\n",
        "# same we have to do for test\n",
        "test_X= test[prediction_var] # taking test data inputs\n",
        "test_y =test.diagnosis   #output value of test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Duyjr72PDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM2fcmKxCWbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TWMD9W5CWhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwbYfrW_CWpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9u52GcD0Yep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X = train[prediction_var]# taking the training data input \n",
        "train_y=train.diagnosis# This is output of our training data\n",
        "# same we have to do for test\n",
        "test_X= test[prediction_var] # taking test data inputs\n",
        "test_y =test.diagnosis   #output value of test dat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhA0W3kV0UIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now split our data into train and test\n",
        "train, test = train_test_split(tumor, test_size = 0.3)# in this our main data is split into train and test\n",
        "# we can check their dimension\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cug66ko30Ut9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Drafts!!!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Jqcp0S_oeqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oLQ-5Doxhr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert dfs to arrays\n",
        "inputs = np.array(inputs, dtype = \"float32\")\n",
        "labels = np.array(labels, dtype= \"float32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SCZ_KUC3NhT",
        "colab_type": "text"
      },
      "source": [
        "Transform the data into arrays and then convert to tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khal8ZPkWBiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert arrays to tensors\n",
        "inputs = torch.from_numpy(inputs)\n",
        "labels = torch.from_numpy(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpBDqvqvKfXb",
        "colab_type": "text"
      },
      "source": [
        "Need to divide the data into a training and test set. Define a function to split the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDzaQRaUGYIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#Define dataset\n",
        "train_ds = TensorDataset(inputs, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-0-uTOmFcI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define data loader\n",
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R5PbNLcKiUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DataLoader is typically used in a for-in loop\n",
        "for xb, yb in train_dl:\n",
        "  print(xb)\n",
        "  print(yb)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wz4JBwBOdRS",
        "colab_type": "text"
      },
      "source": [
        "**nn.Linear**\n",
        "\n",
        "nn.Linear can be used to autmoatically initialize the weights and biases, rather than having to do so manually\n",
        "\n",
        "**I have to assess whether this is actually working. I'm concerned with how to set the nn.Linear functions. Also, I think I need to apply softmax to turn into a proabability.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LMdi7pFKvaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model\n",
        "model = nn.Linear(10, 569)\n",
        "print(model.weight)\n",
        "print(model.bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mppP1z69KwW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters - returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix.\n",
        "list(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9skJIgAaKwf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions\n",
        "preds = model(inputs)\n",
        "preds "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAwQTCZgVpOe",
        "colab_type": "text"
      },
      "source": [
        "**Loss Function**\n",
        "\n",
        "Instead of defining a loss function manually, we can use the built-in loss function mse_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N90p6j5SOYnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import nn.functional\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#These package contains many useful loss functions and several other utilities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6Nhna2vVuxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function\n",
        "loss_fn = F.mse_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0CYUrRFVw3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the loss for the current predictions of our model\n",
        "loss = loss_fn(model(inputs), labels)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgjxSs--VzP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define optimizer\n",
        "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
        "\n",
        "#The model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. \n",
        "#We can also specify a learning rate which controls the amount by which the parameters are modified"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03XOq2qWV95c",
        "colab_type": "text"
      },
      "source": [
        "**Train the Model**\n",
        "\n",
        "We will train the model with the same steps but using batches of data. The utility function **fit** trains the model for a given number of epochs\n",
        "\n",
        "1.   Generate predictions\n",
        "2.   Calculate the loss\n",
        "3.   Compute the gradients w.r.t the weights and biases\n",
        "4.   Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "5.   Reset the gradients to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4lMKewAV9O5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to train the model\n",
        "def fit(num_epochs, model, loss_fn, opt):\n",
        "  \n",
        "  # Repeat for given number of epochs:\n",
        "  for epoch in range(num_epochs):\n",
        "    \n",
        "    #Train with batches of data\n",
        "    for xb, yb in train_dl:\n",
        "      \n",
        "      # 1. Generate predictions\n",
        "      pred = model(xb)\n",
        "      \n",
        "      # 2. Calculate loss\n",
        "      loss = loss_fn(pred, yb)\n",
        "      \n",
        "      # 3. Compute Gradients\n",
        "      loss.backward()\n",
        "      \n",
        "      # 4. Update parameters using gradients\n",
        "      opt.step()\n",
        "      \n",
        "      # 5. Reset the gradients to zero\n",
        "      opt.zero_grad()\n",
        "      \n",
        "    # Print the progress\n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print('Epoch [{}/{}, Loss: {:.4f}' .format(\n",
        "          epoch+1, num_epochs, loss.item()))\n",
        "      \n",
        "# We use the data loader defined earlier to get batches of data for every iteration\n",
        "# Instead of updating parameters (weights and biases) manually, we use opt.step to perform the update, and opt.zero_grad to reset the gradients to zero\n",
        "# We've also added a log statement which prints the loss from the last batch of data for every 10th epoch, to track the progress of training. \n",
        "# loss.item returns the actual value stored in the loss tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKK9d7BfV756",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model for 100 epochs\n",
        "fit(100, model, loss_fn, opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5vzP4p4WH3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate predictions to verify that our model is close to our targets\n",
        "preds = model(inputs)\n",
        "preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX8tYakHWKYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compare with labels\n",
        "labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "culWhJRvNdYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##DRAFTS BELOW######"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaQkdOaqmmdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "  \"\"\"Feedforward neural network with 1 hidden layer\"\"\"\n",
        "  def __init__(self, in_size, hidden_size, out_size):\n",
        "      super().__init__()\n",
        "      # hidden layer\n",
        "      self.linear1 = nn.Linear(in_size, hidden_size)\n",
        "      # output layer\n",
        "      self.linear2 = nn.Linear(hidden_size, out_size)\n",
        "    \n",
        "  def forward(self, xb):\n",
        "      # Get intermediate outputs using hidden layers\n",
        "      out = self.linear1(xb)\n",
        "      # Apply activation function\n",
        "      out = F.relu(out)\n",
        "      # Get predictions using output layer\n",
        "      out = self.linear2(out)\n",
        "      return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f9wMYptmolv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 9\n",
        "num_classes = 2\n",
        "\n",
        "model = MnistModel(input_size, hidden_size=3, # 3 nodes  \n",
        "                   out_size=num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjcBJYSCms-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in model.parameters():\n",
        "  print(t.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS1weW31nKt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for xb, yb in train_dl:\n",
        "    outputs = model(xb)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    print('Loss:', loss.item())\n",
        "    break\n",
        "\n",
        "print('Outputs.shape:', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOwLFejEibvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model with one hidden layer\n",
        "model = nn.Sequential(nn.Linear(9, 3),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(3, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Set 5 epochs to start\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for inputs, labels in trainloader:\n",
        "       \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(images)\n",
        "        loss = criterion(output, labels) # Loss calculated from the output compard to the labels \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVdnCM4rMGK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a model\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(9, 3)\n",
        "      self.fc2 = nn.Linear(3, 3)\n",
        "      self.fc4 = nn.Linear(3, 2)\n",
        "      \n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = F.relu(self.fc3(x))\n",
        "      x = F.log_softmax(self.fc4(x), dim=1)\n",
        "      \n",
        "      return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWKbBJZaZ1dB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Classifier()\n",
        "\n",
        "images = next(iter(testloader))\n",
        "\n",
        "# Get the class probabilities \n",
        "ps = torch.exp(model(images))\n",
        "\n",
        "# Make sure the shape is appropriate, we should get 10 class probabilities for 64 examples\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCvyrSa0Osuy",
        "colab_type": "text"
      },
      "source": [
        "# **Create Further Subgroups Based on Characteristics and do Multi-Class Classification**"
      ]
    }
  ]
}
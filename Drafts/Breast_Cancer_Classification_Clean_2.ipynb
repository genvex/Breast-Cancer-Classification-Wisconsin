{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Breast Cancer Classification_Clean_2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghutch/Breast-Cancer-Classification-Wisconsin/blob/master/Breast_Cancer_Classification_Clean_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg5c00Qa8jJD",
        "colab_type": "text"
      },
      "source": [
        "# **Breast Cancer Classification**\n",
        "\n",
        "**Author:** Meg Hutch\n",
        "\n",
        "**Date:** October 22, 2019\n",
        "\n",
        "\n",
        "**Objective:** Classify Breast Cancer Tumnors as Malignant or Benign from the Breast Cancer Wisconin Dataset downloaded fromn https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
        "\n",
        "**Additional reference:** https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
        "\n",
        "**Dataset:** The following is the given data descriptions: \n",
        "\n",
        "**Attribute Information:**\n",
        "\n",
        "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
        "\n",
        "1.   ID Number\n",
        "2.   Diagnosis (M = malignant, B = benign)\n",
        "\n",
        "Ten real-valued features are computed for each cell nucleus (3-32):\n",
        "\n",
        "3.  radius (mean of distances from center to points on the perimeter)\n",
        "4.  texture (standard deviation of gray-scale values)\n",
        "5.  perimeter\n",
        "6.  area\n",
        "7.  smoothness (local variation in radius lengths)\n",
        "8.  compactness (perimeter^2 / area - 1.0)\n",
        "9.  concavity (severity of concave portions of the contour)\n",
        "10. concave points (number of concave portions of the contour)\n",
        "11. symmetry\n",
        "12. fractal dimension (\"coastline approximation\" - 1)\n",
        "\n",
        "The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj7CQe7jKD2U",
        "colab_type": "text"
      },
      "source": [
        "**WIP Updates**\n",
        "\n",
        "**11.01.2019: MH implemented the code for logisitic regression, will need to add random forest, and also ensure the neural network is okay**\n",
        "\n",
        "**11.04.2019: MH is not sure if I need the train_x or val_x to also be converted to long format ; I'm having problems getting the models to run due to dimension problems**\n",
        "\n",
        "**11.05.2019: MH will try revising the code for xb, yb and in regards to the data loader -- I think this is the problem. First though, I will save what I've done to github**\n",
        "\n",
        "**Update: Can't figure out how to upload to github, but I created a cleaner version of this code. I'm thinking about removing the validaiton set, since I think this reduces the size, but I'm a bit weary of doing so.**\n",
        "\n",
        "**Need to also implement random forest classifier**\n",
        "\n",
        "**11.07.2019: I implemented random forest classifier and am created a Clean_2 where I'm going to remove the seperate splitting function from the neural networks so that I am ensuring that I'm testing all on the same set. I am also going to examine the distributions of the split -- this may be imbalanced due to the high Random Forest Accuracy**\n",
        "\n",
        "**Need to analyze evaluation metrics for the neural network. Check this site: https://github.com/betogaona7/dermatologist-AI/blob/master/dermatologist_AI.ipynb**\n",
        "\n",
        "**11.08.2019: It wouldn't hurt to re-evaluate everything; especially since AUC for neural network is pretty low, but I think I have managed to implement all of the pieces**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEngWwvv8imx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import seaborn as sns\n",
        "\n",
        "# Connect Colab to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLEnXnzk-lqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Data\n",
        "tumor = pd.read_csv('/content/drive/My Drive/Projects/Breast_Cancer_Wisconsin/data.csv')\n",
        "\n",
        "# View data\n",
        "tumor.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw7TlnDfAiZH",
        "colab_type": "text"
      },
      "source": [
        "# **Explore Data**\n",
        "\n",
        "First, we will examine the worst values obtained from each patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x64Itdjz_6hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor.diagnosis.value_counts().plot(kind=\"bar\")\n",
        "count_dx = tumor.groupby(['diagnosis']).size()\n",
        "print('Total Number of Patients:', len(tumor.index))\n",
        "print('Number Diagnosed:', count_dx)\n",
        "print('Percent Benign: {:.1f}'.format(357/len(tumor.index)))\n",
        "print('Percent Malignant: {:.1f}'.format(212/len(tumor.index)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZiXuempE_9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new dataframe to just contain columns of interset\n",
        "tumor_plots = tumor[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "\n",
        "# Generically define how many plots along and across\n",
        "ncols = 3\n",
        "nrows = int(np.ceil(len(tumor_plots.columns) / (1.0*ncols)))\n",
        "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10, 10))\n",
        "\n",
        "# Lazy counter so we can remove unwanted axes\n",
        "counter = 0\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "\n",
        "        ax = axes[i][j]\n",
        "\n",
        "        # Plot when we have data\n",
        "        if counter < len(tumor_plots.columns):\n",
        "\n",
        "            ax.hist(tumor_plots[tumor_plots.columns[counter]], bins=50, color='blue', alpha=0.5, label='{}'.format(tumor_plots.columns[counter]))\n",
        "            ax.set_xlabel('x')\n",
        "            ax.set_ylabel('PDF')\n",
        "            leg = ax.legend(loc='upper left')\n",
        "            leg.draw_frame(False)\n",
        "\n",
        "        # Remove axis when we no longer have data\n",
        "        else:\n",
        "            ax.set_axis_off()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzb8q0UFN8Gz",
        "colab_type": "text"
      },
      "source": [
        "# **Correlations for Feature Selection**\n",
        "\n",
        "I'll eventually have to learn how to look into this more, as of now, I'm just going to include all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiyLMuzwIBoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic correlogram\n",
        "#sns.pairplot(tumor_plots)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js2xeo28UYjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGMu8lrOT3NP",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "We will first try and assess classification using a simple logistic regression - this will also serve as a bench mark once we develop our neural network classifier\n",
        "\n",
        "These steps were followed from the following tutorial: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad_QNUYJUEeh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Packages \n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-XsYtW6VGiZ",
        "colab_type": "text"
      },
      "source": [
        "Create dummy variables for diagnoses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx-r5-39f3oS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tumor['diagnosis'] = tumor.diagnosis.map({'B':0, 'M':1})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zX8HZEAUpr0",
        "colab_type": "text"
      },
      "source": [
        "Create data frames into features and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saU_eKbKUPJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create x to represent the input features; y is the label; \n",
        "x = tumor[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]\n",
        "y = tumor.diagnosis # This is output of our training data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaRAWZErUmWK",
        "colab_type": "text"
      },
      "source": [
        "Split the data into testing and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcRht6w7Ukfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVJUA0cGaYUh",
        "colab_type": "text"
      },
      "source": [
        "The way I named the labels above may be a bit confusing. \n",
        "\n",
        "But the way the splits above work, train = training data, test = testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5aG5-vDaWzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assess the trainig and testing sets previously created\n",
        "print('Training Features Shape:', X_train.shape)\n",
        "print('Training Labels Shape:', y_train.shape)\n",
        "print('Testing Features Shape:', X_test.shape)\n",
        "print('Testing Labels Shape:', y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrT1iinptJ8k",
        "colab_type": "text"
      },
      "source": [
        "Examine the Class Distributions - We can see that in both the testing and the training set, 37% of the cases are malignant. I feel like this a good split because within the whole dataset there is 40% malignant cases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puXbH1LbtLgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Malignant = 1; Count the number of malignant and determine the percentage (traning has 426 values, testing has 143)\n",
        "print('Malginant Cases in Training Set:', np.count_nonzero(y_train == 1))\n",
        "print('Malginant Cases in Testing Set:', np.count_nonzero(y_test == 1))\n",
        "\n",
        "# Percents\n",
        "print('% Malginant Cases in Training Set:', round(np.count_nonzero(y_train == 1)/426*100,2))\n",
        "print('% Malginant Cases in Testing Set:', round(np.count_nonzero(y_test == 1)/143*100,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcupQtxvUv4Y",
        "colab_type": "text"
      },
      "source": [
        "Develop the logistic regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXGPVFGJUwry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate the model (using default parameters)\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# fit the model with data\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXQpnqyzVAo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_5ROBDWlZ1",
        "colab_type": "text"
      },
      "source": [
        "**Model Evaulation using Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E6ZqC4EW4u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQuiYl2CXDfn",
        "colab_type": "text"
      },
      "source": [
        "The confusion matrix generated abouve is in the form of an array. Diagonal values represent accurate predictions, while non-diagnonal elements are inaccurate predictions. The diagnoal starting with the top left to the bottom right hand corner are the actual predictions, while the bottom left corner to the top right corner are incorrect predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrgS-ANvXbks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZKVIx5-aqCY",
        "colab_type": "text"
      },
      "source": [
        "**ROC**\n",
        "\n",
        "The Reciever Operating Characteristic (ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFijxjja1zW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "print(round(auc,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unSP50nHbQtq",
        "colab_type": "text"
      },
      "source": [
        "# **Random Forest Classification**\n",
        "\n",
        "I am following the tutorial here: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0 and https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/\n",
        "\n",
        "We already split the data into training and testing above. We will use this same split for the random forest model.\n",
        "\n",
        "**Train the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRXPzQNIX6fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the model we are using\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate model with 1000 decision trees\n",
        "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQCVLc9AgRTH",
        "colab_type": "text"
      },
      "source": [
        "**Make Predictions on the Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SEB0ncxgQ78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the forest's predict method on the test data\n",
        "predictions = rf.predict(X_test)\n",
        "# Probabilities for each class\n",
        "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the absolute errors\n",
        "errors = abs(predictions - y_test)\n",
        "# Print out the mean absolute error (mae)\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxrHZgEgjuTp",
        "colab_type": "text"
      },
      "source": [
        "Results indicate that our estimate is off by 0.06\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-91kRggjKwo",
        "colab_type": "text"
      },
      "source": [
        "We make class predictions (predict) as well as predicted probabilities (predict_proba) to calculate the ROC AUC. Once we have the testing predictions, we can calculate the ROC AUC. https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9L249XWjnRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Calculate roc auc\n",
        "roc_value = roc_auc_score(y_test, rf_probs)\n",
        "\n",
        "roc_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRZ8mI9gn2BE",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the Algorithm**\n",
        "\n",
        "For classification problems the metrics used to evaluate an algorithm are accuracy, confusion matrix, precision recall, and F1 values. Execute the following script to find these values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLPWpNNvtwKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYgUVze4tza4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cross Fold Validation\n",
        "rfc_cv_score = cross_val_score(rf, x, y, cv=10, scoring='roc_auc') #https://medium.com/@hjhuney/implementing-a-random-forest-classification-model-in-python-583891c99652 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdmiRpz6kY17",
        "colab_type": "text"
      },
      "source": [
        "Below, we can examine the confusion matrix, the classification report contraining precision, recall, f1-score, and supoprt, All AUC scores, and the Mean AUC Score.\n",
        "\n",
        "The **confusion matrix** is useful for giving false positives and false negatives. \n",
        "\n",
        "The **ROC Curve** plots out the true positive rate vs the false positive rate at various thresholds\n",
        "\n",
        "The **Roc AUC Scoring** used in the cross-validaiton model shows the area under the ROC curve \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZRHJqGcv9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"=== Confusion Matrix ===\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== Classification Report ===\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print('\\n')\n",
        "print(\"=== All AUC Scores ===\")\n",
        "print(rfc_cv_score)\n",
        "print('\\n')\n",
        "print(\"=== Mean AUC Score ===\")\n",
        "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())\n",
        "print('\\n')\n",
        "print(\"=== Accuracy Score ===\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djc7O1sUl56l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Probabilities for each class\n",
        "# Note from scikit learn re: fpr and tpr: \"Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr, which are sorted in reversed order during their calculation.\"\n",
        "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  rf_probs)\n",
        "auc = metrics.roc_auc_score(y_test, rf_probs)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yia9sVO-l2as",
        "colab_type": "text"
      },
      "source": [
        "The Random forest classifier has a slightly high accuracy than logistic regression 92% vs 94%. Additionally, for the Random Forest Classifier the AUC was 0.984 after 10 fold cross validation, though the first run had an AUC of .99. For the Logistic Regression Classifier, was 0.984, the same as after the 10 fold cross validation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBP42op2Oaca",
        "colab_type": "text"
      },
      "source": [
        "## **PyTorch Neural Network for Classification**\n",
        "\n",
        "We will evaluate the use of a neural network classifier on the testing data as developed above and compare the outputs to the logistic regression and random forest classifiers above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ld6UECBRrk93",
        "colab": {}
      },
      "source": [
        "# Import PyTorch packages\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zf-Y2-ZnF9ir"
      },
      "source": [
        "**Format the Training Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRXTKvao2Gy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_train, dtype = \"float32\")\n",
        "yb = np.array(y_train, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "trainloader = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Training Loader\n",
        "trainloader = DataLoader(trainloader, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ak9tHX6MZXx",
        "colab_type": "text"
      },
      "source": [
        "**Format the Testing Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StPU4Ytl2hA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert data into arrays\n",
        "xb = np.array(X_test, dtype = \"float32\")\n",
        "yb = np.array(y_test, dtype= \"float32\")\n",
        "\n",
        "# Convert arrays into tensors\n",
        "xb = torch.from_numpy(xb)\n",
        "yb = torch.from_numpy(yb)\n",
        "\n",
        "#Combine the arrays \n",
        "test = TensorDataset(xb, yb) \n",
        "\n",
        "# Define the batchsize\n",
        "batch_size=25\n",
        "\n",
        "# Training Loader\n",
        "testloader = DataLoader(test, \n",
        "                         batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqdQLEFfM5eE",
        "colab_type": "text"
      },
      "source": [
        "**Create Neural Network Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLZs2xTcTyXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model with one hidden layer\n",
        "model = nn.Sequential(nn.Linear(10, 5),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(5, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Could also use Adam optimizer; similar to stochastic gradient descent, but uses momentum which can speed up the actual fitting process, and it also adjusts the learning rate for each of the individual parameters in the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Set 100 epochs to start\n",
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for xb, yb in trainloader:\n",
        "\n",
        "        # Flatten yb\n",
        "        #yb = yb.view(yb.shape[0], -1)\n",
        "        \n",
        "        # Clear the gradients, do this because gradients are accumulated\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Training pass\n",
        "        output = model.forward(xb)\n",
        "        loss = criterion(output, yb.long()) # Loss calculated from the output compared to the labels \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item() # loss.item() gets the scalar value held in the loss. Running_loss = 0, \n",
        "        # += notation, says \"Add a value and the variable and assigns the result to that variable.\" So, adds the running_loss (0) with loss.item and assigns to running_loss\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oY7eGLJUnlb",
        "colab_type": "text"
      },
      "source": [
        "The goal of validation is to measure the model's performance on data that isn't part of the training set. Performance here is up to the developer to define though. Typically, this is just accuracy, the percentage of classes the network predicted correctly. \n",
        "\n",
        "First, do a forward pass with one batch from the test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UexgheavO9L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xb, yb = next(iter(testloader))\n",
        "\n",
        "# Get the class probabilities \n",
        "ps = torch.exp(model(xb))\n",
        "\n",
        "# Make sure the shape is appropriate\n",
        "print(ps.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BOkgVzsVJfl",
        "colab_type": "text"
      },
      "source": [
        "With the probabilities, we can get the most likely class using the ps.topk method. This returns the k highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top-k values and the top-k indices. If the highest value is the first element, we'll get back 4 as the index. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HMCtu70PVgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_p, top_class = ps.topk(1, dim=1)\n",
        "# Look at the most likely classes for the first 20 examples\n",
        "print(top_class[:20,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJWhhdMDKZXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uawVcX1k_7Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps[:20] #we can see how to correpsonding probabilities were classified above"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uIABTsjVYz0",
        "colab_type": "text"
      },
      "source": [
        "Now we can check if the predicted classes match the labels. This is simple to do by equating top_class and labels, but we have to be careful of the shapes. To get the equality to work out the way we want, top_class and the labels (yb) must have the same shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPw1mTkRE5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "equals = top_class == yb.view(*top_class.shape) \n",
        "equals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFCu3Hdj9kOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_class = top_class.view(*yb.shape) \n",
        "top_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QeBWPlbEhJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cast top_class to float32\n",
        "top_class = top_class.to(dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H0McJLVVoi1",
        "colab_type": "text"
      },
      "source": [
        "Now we need to calculate the correct predictions. \n",
        "\n",
        "equals has binary values, either 0 or 1. This means that if we just sum up all the values and divide by the total number of values, we get the percentage of correct predictions. This is the same operation as taking the mean, so we can get the accuracy with a call to torch.mean. \n",
        "\n",
        "So we'll need to convert equals to a float tensor. Note that when we take torch.mean it returns a scalar tensor, to get the actual value as a float we'll need to do accuracy.item()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiSuP4tGSPty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "print(f'Accuracy: {accuracy.item()*100}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIRwVHjd50yp",
        "colab_type": "text"
      },
      "source": [
        "**Evaulating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSPflPpS53Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.vision import *\n",
        "roc_auc = auc_roc_score(yb, top_class)\n",
        "roc_auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v68E7642I5wb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_class.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMrBHZ5WFVgN",
        "colab_type": "text"
      },
      "source": [
        "**Plot ROC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooNdgqzGFXEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(yb, top_p, pos_label=1)\n",
        "\n",
        "# Compute ROC area\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('ROC area is {0}'.format(roc_auc))\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
        "plt.xlim([-0.01, 1.0])\n",
        "plt.ylim([0.0, 1.01])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1ynFim8i_iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculating True Positive Rate (Sensitivity) and False Positive Rate (Sensitivity)\n",
        "# https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
        "\n",
        "CM = confusion_matrix(yb, top_class)\n",
        "\n",
        "TN = CM[0][0] # True Negative is high - is this because our model has greater sensitivty\n",
        "FN = CM[1][0] # False Negative\n",
        "TP = CM[1][1] # True Positive\n",
        "FP = CM[0][1] # False POsitive "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMSzg8ZQkS_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FN"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
